{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trizzole/HelloAI/blob/main/Assignment5_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat ~/.keras/keras.json\n",
        "!git clone \"https://github.com/kartoone/nn3\"\n",
        "!cat nn3/keras.json > ~/.keras/keras.json\n",
        "!pip uninstall -y keras\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow==2.2\n",
        "!pip install keras==2.2.4\n",
        "!pip install pydot-ng\n",
        "!pip install theano==0.8\n",
        "%cd nn3/src\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zufu4wflP581",
        "outputId": "726cb4c3-4568-49ca-b630-e40bb14e7218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nn3'...\n",
            "remote: Enumerating objects: 174, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 174 (delta 35), reused 26 (delta 11), pack-reused 109\u001b[K\n",
            "Receiving objects: 100% (174/174), 94.37 MiB | 20.34 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "Updating files: 100% (107/107), done.\n",
            "Found existing installation: keras 2.11.0\n",
            "Uninstalling keras-2.11.0:\n",
            "  Successfully uninstalled keras-2.11.0\n",
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.2\n",
            "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.4.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.15.0)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (3.3.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (0.2.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.6/454.6 KB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (2.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.21.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (0.38.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.51.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.6.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.1)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, scipy, keras-preprocessing, h5py, gast, cachetools, google-auth, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.16.0\n",
            "    Uninstalling google-auth-2.16.0:\n",
            "      Successfully uninstalled google-auth-2.16.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-vis 0.4.1 requires keras, which is not installed.\n",
            "xarray-einstats 0.5.1 requires scipy>=1.6, but you have scipy 1.4.1 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.25 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-4.2.4 gast-0.3.3 google-auth-1.35.0 h5py-2.10.0 keras-preprocessing-1.1.2 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.5/312.5 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (6.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-applications, keras\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot-ng in /usr/local/lib/python3.8/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from pydot-ng) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting theano==0.8\n",
            "  Downloading Theano-0.8.0.zip (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.15.0)\n",
            "Building wheels for collected packages: theano\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-0.8.0-py3-none-any.whl size=2722137 sha256=30797b5398b53c2d289aee6649ffca97b8d98194a7ce6350f363bf6d82ec11e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/fa/00/251dbd5e561228175439939ac67c8c17484f615a245c2479d8\n",
            "Successfully built theano\n",
            "Installing collected packages: theano\n",
            "Successfully installed theano-0.8.0\n",
            "/content/nn3/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/.keras/keras.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apRaTZgACf0m",
        "outputId": "6f8cbb48-7055-46c3-e094-d106e27d3246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"floatx\": \"float32\",\n",
            "    \"epsilon\": 1e-07,\n",
            "    \"backend\": \"theano\",\n",
            "    \"image_dim_ordering\": \"th\",\n",
            "    \"image_data_format\": \"channels_first\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import network3\n",
        "from network3 import Network\n",
        "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
        "training_data, validation_data, test_data = network3.load_data_shared()\n",
        "mini_batch_size = 10\n",
        "net = Network([\n",
        "        FullyConnectedLayer(n_in=784, n_out=100),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
        "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgOjEgKjVp0o",
        "outputId": "04fdb27a-ceb2-4e9d-e265-4b9f2be8be84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
            "to set the GPU flag to False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "WARNING:theano.tensor.blas:We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 0.9245\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9187000000000002\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 0.9448000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9397000000000002\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 0.955\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9497000000000002\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 0.9614000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9572\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 0.9663\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9621000000000002\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 0.9691000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9661000000000002\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 0.9707\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9682000000000001\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 0.9719000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9708000000000001\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 0.9725000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9726000000000001\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 0.9738000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9735\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 0.9741000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9740000000000001\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 0.9746\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9748\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 0.9748000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9757\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 0.9752000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9763\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 0.9754\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9766\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 0.9755\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9769000000000001\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 0.976\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.977\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 0.9763000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9768000000000002\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 0.9765\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 0.9768000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9780000000000001\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 0.9771\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9781000000000002\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 0.9776000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9784\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 0.978\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9787000000000001\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 0.9781000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9788000000000001\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 0.9782000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.979\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 0.9783\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9791\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 0.9784\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9789000000000001\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 0.9787\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9789000000000001\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 0.9786\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 0.9787\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9788\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 0.9788000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.979\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 0.9788000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9791000000000001\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 0.9789000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9793000000000001\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 0.9789000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9793000000000002\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 0.979\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9795000000000001\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 0.9791000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9794\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 0.979\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 0.979\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 0.9787\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 0.9787\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 0.9786\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 0.9787\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 0.9788000000000001\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 0.9787\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 0.9786\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 0.9786\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 0.9786\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 0.9785\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 0.9783000000000001\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 0.9783000000000001\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 0.9782000000000001\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 0.9782000000000001\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 0.9782000000000001\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 0.9782000000000001\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 0.9783000000000001\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 0.9782000000000001\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 0.9783000000000001\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 0.9784\n",
            "Finished training network.\n",
            "Best validation accuracy of 97.91% obtained at iteration 189999\n",
            "Corresponding test accuracy of 97.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "(worsti, worsta) = findTroubleSomeImage(net, test_data)\n",
        "print(worsti, worsta)\n",
        "print(test_data[1][270].eval())\n",
        "pixel_data = test_data[0][270].reshape((28,28)).eval()\n",
        "pixel_data = [pix*255 for pix in pixel_data]\n",
        "#print(pixel_data)\n",
        "net.test_mb_outputs(0)\n",
        "#print(test_data[1][1].eval())\n",
        "net.test_mb_outputs(0)\n",
        "plt.figure()\n",
        "plt.imshow(pixel_data, cmap=\"gray_r\")\n"
      ],
      "metadata": {
        "id": "5gWTjX21Hc_r",
        "outputId": "8b34adbf-0070-4251-e408-bd42ade83a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 0.51716435\n",
            "3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbb008b7dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assume batch size of 10\n",
        "import numpy as np\n",
        "def findTroubleSomeImage(net, test_data, batchsize=10):\n",
        "  batches = int(len(test_data[0][8].eval())/batchsize)\n",
        "  worsta = 1.0\n",
        "  worsti = 0\n",
        "  for i in range(batches):\n",
        "    output = net.test_mb_outputs(i)\n",
        "    for a in output:\n",
        "      if np.max(a) < worsta:\n",
        "        worsta = np.max(a)\n",
        "        worsti = i\n",
        "  return(worsti, worsta)"
      ],
      "metadata": {
        "id": "j5os1wk9TcVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show theano"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzPpIsXuF6Eb",
        "outputId": "51ad5ae7-091b-45ba-ca1c-eec23699bb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Theano\n",
            "Version: 0.8.0\n",
            "Summary: Optimizing compiler for evaluating mathematical expressions on CPUs and GPUs.\n",
            "Home-page: http://deeplearning.net/software/theano/\n",
            "Author: LISA laboratory, University of Montreal\n",
            "Author-email: theano-dev@googlegroups.com\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: numpy, scipy, six\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import network3\n",
        "from network3 import Network\n",
        "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
        "training_data, validation_data, test_data = network3.load_data_shared()\n",
        "mini_batch_size = 10\n",
        "\n",
        "import time\n",
        "\n",
        "net = Network([\n",
        "  ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
        "              filter_shape=(3, 1, 5, 5), \n",
        "              poolsize=(2, 2)),\n",
        "  #FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
        "  SoftmaxLayer(n_in=3*12*12, n_out=10)], mini_batch_size)\n",
        "start = time.time()\n",
        "net.SGD(training_data, 60, mini_batch_size, 0.1, \n",
        "    validation_data, test_data)   \n",
        "finish = time.time()\n",
        "elapsed = finish - start\n",
        "print(elapsed/60 + \" minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97kzimKFEZPm",
        "outputId": "3de32c06-9bdb-4611-c37b-122fb20b74c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 0.9489000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9466000000000001\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 0.9619000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9603\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 0.9667\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9661000000000002\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 0.9713\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9686000000000001\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 0.9735\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9705\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 0.975\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9723\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 0.9762000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9739000000000001\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 0.9766000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9753000000000001\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 0.9772000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9756\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 0.9771000000000002\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 0.9775000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9763\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 0.9775\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 0.9779000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 0.978\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.977\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 0.9784\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9773000000000001\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 0.9784\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9773000000000001\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 0.9787\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9772000000000001\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 0.9787\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 0.9787\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9769\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 0.9787\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 0.9788\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771000000000002\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 0.9792000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9771000000000002\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 0.9792000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9773000000000001\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 0.9792000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9773000000000001\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 0.9793000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9776000000000001\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 0.979\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 0.9789000000000001\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 0.9788\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 0.9787\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 0.9785\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 0.9785\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 0.9784\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 0.9787\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 0.9787\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 0.9788000000000001\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 0.9788000000000001\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 0.9787\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 0.9786\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 0.9786\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 0.9784\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 0.9785\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 0.9786000000000001\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 0.9789\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 0.979\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 0.979\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 0.979\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 0.9791000000000001\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 0.9792000000000002\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 0.9792000000000001\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 0.9794\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.978\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 0.9793000000000002\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 0.9793000000000002\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 0.9792000000000001\n",
            "Finished training network.\n",
            "Best validation accuracy of 97.94% obtained at iteration 284999\n",
            "Corresponding test accuracy of 97.80%\n",
            "240.68761110305786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import network3\n",
        "from network3 import Network\n",
        "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
        "training_data, validation_data, test_data = network3.load_data_shared()\n",
        "mini_batch_size = 10\n",
        "\n",
        "import time\n",
        "\n",
        "net = Network([\n",
        "  ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
        "              filter_shape=(20, 1, 5, 5), \n",
        "              poolsize=(2, 2)),\n",
        "  FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
        "  SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
        "start = time.time()\n",
        "net.SGD(training_data, 60, mini_batch_size, 0.1, \n",
        "    validation_data, test_data)   \n",
        "finish = time.time()\n",
        "elapsed = finish - start\n",
        "print(elapsed)\n",
        "print(str(elapsed/60) + \" minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6uWKrRReJCI5",
        "outputId": "c4fec6ef-0906-4bd2-81c7-effdb14a7ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 0.9374000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9326000000000001\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 0.9582\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9583000000000002\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 0.9675000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9692000000000001\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 0.9723\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9732000000000001\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 0.9761\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9761\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 0.978\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9783\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 0.979\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9793\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 0.9801\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9803\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 0.9814\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9811\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 0.9814\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9813000000000001\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 0.982\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9820000000000001\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 0.9824\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9827\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 0.9829\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9829000000000001\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 0.9837\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9837\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 0.984\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9845000000000002\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 0.9843999999999999\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9845\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 0.9847\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9847\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 0.9847\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9849000000000001\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 0.9848000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9853000000000001\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 0.9852000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9856\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 0.9853\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9855\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 0.9855000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9854\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 0.9856\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9853000000000001\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 0.9857\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9853000000000001\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 0.9859000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9853999999999999\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 0.9866\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9853999999999999\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 0.9868000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9855\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 0.9869000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9853\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 0.9869000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9853999999999999\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 0.9869000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9855\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 0.987\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9856\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 0.9872000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9856\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 0.9873000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9859\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 0.9874\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.986\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 0.987\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 0.9872000000000001\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 0.9874\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9859\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 0.9873999999999999\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 0.9873999999999999\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 0.9873999999999999\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 0.9875\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.986\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 0.9874\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 0.9875\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.986\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 0.9875\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9861\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 0.9875\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9863\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 0.9875\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9865\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 0.9875\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9865\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 0.9875\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9865\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 0.9877\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9867\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 0.9877\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9866000000000001\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 0.9876\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 0.9876\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 0.9875\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 0.9876\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 0.9876\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 0.9875999999999999\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 0.9877\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9867\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 0.9877\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9867\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 0.9877\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9867999999999999\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 0.9877\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9869000000000001\n",
            "Finished training network.\n",
            "Best validation accuracy of 98.77% obtained at iteration 299999\n",
            "Corresponding test accuracy of 98.69%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0fc2f3792733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mfinish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinish\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
          ]
        }
      ]
    }
  ]
}